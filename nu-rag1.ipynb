{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 128\u001b[0m\n\u001b[1;32m     76\u001b[0m ideas \u001b[38;5;241m=\u001b[39m [  \n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreminder to cancel - nu capitalist - cd subscription scheme\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdea of this 2 write(w) - hmmm - like to write as 2 speak - like when u think it has its own vibe but when u write u get the closest to that idea making process or at least try to tap into it like other mediums\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaptive Fashion is kind of like revising your look…adapt to like what the people want…give’m what they want… adapt to trends if there are aspects you like ..as in the comfort of parachute pants :)\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m    126\u001b[0m ]  \n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Initialize with your ideas\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m rag \u001b[38;5;241m=\u001b[39m \u001b[43mIdeaRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mideas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m    131\u001b[0m response \u001b[38;5;241m=\u001b[39m rag\u001b[38;5;241m.\u001b[39mgenerate_response(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can shorthand writing improve creative thinking?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mIdeaRAG.__init__\u001b[0;34m(self, ideas)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/flan-t5-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Initialize Pinecone\u001b[39;00m\n\u001b[1;32m     26\u001b[0m pinecone\u001b[38;5;241m.\u001b[39minit(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOUR_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m, environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOUR_ENV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:74\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires Accelerate: `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers pinecone-client rank-bm25 transformers accelerate bitsandbytes\n",
    "\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "\n",
    "# Initialize components\n",
    "class IdeaRAG:\n",
    "    def __init__(self, ideas):\n",
    "        self.ideas = ideas\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize models with 4-bit quantization\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "        self.generator = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            \"google/flan-t5-small\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(api_key=\"YOUR_KEY\", environment=\"YOUR_ENV\")\n",
    "        self.index = pinecone.Index(\"your-index-name\")\n",
    "        \n",
    "        # Prepare hybrid search\n",
    "        self._prepare_hybrid_search()\n",
    "    \n",
    "    def _prepare_hybrid_search(self):\n",
    "        # Create BM25 corpus\n",
    "        tokenized_ideas = [idea.split() for idea in self.ideas]\n",
    "        self.bm25 = BM25Okapi(tokenized_ideas)\n",
    "        \n",
    "        # Upload ideas to Pinecone\n",
    "        embeddings = self.embedder.encode(self.ideas)\n",
    "        records = [(str(idx), emb.tolist(), {\"text\": idea}) \n",
    "                 for idx, (emb, idea) in enumerate(zip(embeddings, self.ideas))]\n",
    "        self.index.upsert(records)\n",
    "\n",
    "    def _hybrid_search(self, query, top_k=5):\n",
    "        # Vector search\n",
    "        query_embedding = self.embedder.encode(query).tolist()\n",
    "        vector_results = self.index.query(query_embedding, top_k=top_k*2, include_metadata=True)\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_indices = np.argsort(bm25_scores)[-top_k*2:][::-1]\n",
    "        \n",
    "        # Combine results\n",
    "        combined = [(match['score'], match['metadata']['text']) for match in vector_results['matches']]\n",
    "        combined += [(bm25_scores[i], self.ideas[i]) for i in bm25_indices]\n",
    "        \n",
    "        # Deduplicate and sort\n",
    "        unique_results = {text: score for score, text in combined}\n",
    "        return sorted(unique_results.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        # Retrieve relevant ideas\n",
    "        context = [text for text, _ in self._hybrid_search(query)]\n",
    "        \n",
    "        # Generate response\n",
    "        input_text = f\"Answer based on these ideas: {', '.join(context)}\\n\\nQuestion: {query}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "ideas = [ \"1,2,3,4,5,6,7,8\" ]\n",
    "# Initialize with your ideas\n",
    "rag = IdeaRAG(ideas)\n",
    "\n",
    "# Example usage\n",
    "response = rag.generate_response(\"How can shorthand writing improve creative thinking?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle version\n",
    "import pickle\n",
    "import pinecone\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load ideas from pickle file\n",
    "with open(\"ideas.pickle\", \"rb\") as f:\n",
    "    ideas = pickle.load(f)  # Ensure it is a list of strings\n",
    "\n",
    "# Initialize components\n",
    "class IdeaRAG:\n",
    "    def __init__(self, ideas):\n",
    "        self.ideas = ideas\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize models with 4-bit quantization\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "        self.generator = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            \"google/flan-t5-small\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(api_key=\"YOUR_KEY\", environment=\"YOUR_ENV\")\n",
    "        self.index = pinecone.Index(\"your-index-name\")\n",
    "        \n",
    "        # Prepare hybrid search\n",
    "        self._prepare_hybrid_search()\n",
    "    \n",
    "    def _prepare_hybrid_search(self):\n",
    "        # Create BM25 corpus\n",
    "        tokenized_ideas = [idea.split() for idea in self.ideas]\n",
    "        self.bm25 = BM25Okapi(tokenized_ideas)\n",
    "        \n",
    "        # Upload ideas to Pinecone\n",
    "        embeddings = self.embedder.encode(self.ideas)\n",
    "        records = [(str(idx), emb.tolist(), {\"text\": idea}) \n",
    "                   for idx, (emb, idea) in enumerate(zip(embeddings, self.ideas))]\n",
    "        self.index.upsert(records)\n",
    "\n",
    "    def _hybrid_search(self, query, top_k=5):\n",
    "        # Vector search\n",
    "        query_embedding = self.embedder.encode(query).tolist()\n",
    "        vector_results = self.index.query(query_embedding, top_k=top_k*2, include_metadata=True)\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_indices = np.argsort(bm25_scores)[-top_k*2:][::-1]\n",
    "        \n",
    "        # Combine results\n",
    "        combined = [(match['score'], match['metadata']['text']) for match in vector_results['matches']]\n",
    "        combined += [(bm25_scores[i], self.ideas[i]) for i in bm25_indices]\n",
    "        \n",
    "        # Deduplicate and sort\n",
    "        unique_results = {text: score for score, text in combined}\n",
    "        return sorted(unique_results.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        # Retrieve relevant ideas\n",
    "        context = [text for text, _ in self._hybrid_search(query)]\n",
    "        \n",
    "        # Generate response\n",
    "        input_text = f\"Answer based on these ideas: {', '.join(context)}\\n\\nQuestion: {query}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=150,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize with loaded ideas\n",
    "rag = IdeaRAG(ideas)\n",
    "\n",
    "# Example usage\n",
    "response = rag.generate_response(\"How can shorthand writing improve creative thinking?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkness\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Load ideas from pickle file\n",
    "with open(\"ideas.pickle\", \"rb\") as f:\n",
    "    ideas = pickle.load(f)\n",
    "\n",
    "class IdeaRAG:\n",
    "    def __init__(self, ideas):\n",
    "        self.ideas = ideas\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Initialize models without quantization\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.generator = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            \"google/flan-t5-base\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # FAISS initialization\n",
    "        self._prepare_hybrid_search()\n",
    "    \n",
    "    def _prepare_hybrid_search(self):\n",
    "        # Create BM25 corpus\n",
    "        tokenized_ideas = [idea.split() for idea in self.ideas]\n",
    "        self.bm25 = BM25Okapi(tokenized_ideas)\n",
    "        \n",
    "        # Create FAISS index\n",
    "        embeddings = self.embedder.encode(self.ideas)\n",
    "        self.idea_vectors = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # Initialize FAISS index (Inner Product similarity)\n",
    "        self.index = faiss.IndexFlatIP(self.idea_vectors.shape[1])\n",
    "        self.index.add(self.idea_vectors)\n",
    "\n",
    "    def _hybrid_search(self, query, top_k=5):\n",
    "        # Vector search with FAISS\n",
    "        query_embedding = self.embedder.encode(query).reshape(1, -1).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, top_k*2)\n",
    "        \n",
    "        # Get FAISS results\n",
    "        vector_results = [(distances[0][i], self.ideas[indices[0][i]]) \n",
    "                         for i in range(len(indices[0]))]\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        bm25_indices = np.argsort(bm25_scores)[-top_k*2:][::-1]\n",
    "        \n",
    "        # Combine results\n",
    "        combined = vector_results + [(bm25_scores[i], self.ideas[i]) for i in bm25_indices]\n",
    "        \n",
    "        # Deduplicate and sort\n",
    "        unique_results = {text: score for score, text in combined}\n",
    "        return sorted(unique_results.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        # Retrieve relevant ideas\n",
    "        context = [text for text, _ in self._hybrid_search(query)]\n",
    "        \n",
    "        # Generate response\n",
    "        input_text = f\"Answer based on these ideas: {', '.join(context)}\\n\\nQuestion: {query}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=250,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize with loaded ideas\n",
    "rag = IdeaRAG(ideas)\n",
    "\n",
    "# Example usage\n",
    "response = rag.generate_response(\"What's connected with ambient music?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe0b40d137d45c0b7f32c5f348c43c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e4fdabe274a658c1d88fd777ae21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c28527f390948d78b52169430eaa055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d283c4392094f02b383fe87d111be34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b580a79bc952441c8c14d77c75f70299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24c23711fc547129392d6f99bbe3d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69542f3d6d04bcdb93908ffa08e3afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905fc20f2a2748789b315560fde33237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d6fc0ba9bb40b1847142c3033e56ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5bebffb8614c34bd92fd5810b3a727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45f1a42f7534d9eadc9ee693db7ad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darkness\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class AdvancedIdeaRAG(IdeaRAG):\n",
    "    def __init__(self, ideas):\n",
    "        super().__init__(ideas)\n",
    "        \n",
    "        # Cross-encoder for re-ranking\n",
    "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=self.device)\n",
    "        \n",
    "        # Summarization pipeline\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0 if self.device == \"cuda\" else -1)\n",
    "        \n",
    "        # Memory for context refinement\n",
    "        self.memory = []\n",
    "\n",
    "    def _mmr_diversity(self, query, retrieved_docs, lambda_param=0.7, top_k=5):\n",
    "        \"\"\"Implements Maximal Marginal Relevance (MMR) for diverse results\"\"\"\n",
    "        query_embedding = self.embedder.encode(query).reshape(1, -1)\n",
    "        doc_embeddings = np.array([self.embedder.encode(doc) for doc in retrieved_docs])\n",
    "        \n",
    "        selected = []\n",
    "        candidates = list(range(len(retrieved_docs)))\n",
    "        \n",
    "        for _ in range(top_k):\n",
    "            if not selected:\n",
    "                idx = np.argmax([np.dot(query_embedding, doc.T) for doc in doc_embeddings])\n",
    "            else:\n",
    "                similarities = np.array([np.dot(doc_embeddings[i], doc_embeddings[selected].T).max() for i in candidates])\n",
    "                scores = lambda_param * np.dot(query_embedding, doc_embeddings[candidates].T).flatten() - (1 - lambda_param) * similarities\n",
    "                idx = candidates[np.argmax(scores)]\n",
    "            \n",
    "            selected.append(idx)\n",
    "            candidates.remove(idx)\n",
    "        \n",
    "        return [retrieved_docs[i] for i in selected]\n",
    "\n",
    "    def _rerank_results(self, query, results):\n",
    "        \"\"\"Uses a cross-encoder to re-rank retrieved results\"\"\"\n",
    "        scores = self.reranker.predict([(query, doc) for doc in results])\n",
    "        return [doc for _, doc in sorted(zip(scores, results), reverse=True)][:5]\n",
    "\n",
    "    def generate_response(self, query):\n",
    "        # Retrieve relevant ideas using hybrid search\n",
    "        retrieved_ideas = [text for text, _ in self._hybrid_search(query, top_k=10)]\n",
    "        \n",
    "        # Apply MMR for diversity\n",
    "        diverse_ideas = self._mmr_diversity(query, retrieved_ideas)\n",
    "        \n",
    "        # Rerank results\n",
    "        top_ideas = self._rerank_results(query, diverse_ideas)\n",
    "        \n",
    "        # Summarize context\n",
    "        context_text = \" \".join(top_ideas)\n",
    "        summary = self.summarizer(context_text, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "        \n",
    "        # Memory augmentation (store past queries)\n",
    "        self.memory.append((query, summary))\n",
    "        \n",
    "        # Generate final response\n",
    "        input_text = f\"Context: {summary}\\n\\nQuestion: {query}\"\n",
    "        inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=250,\n",
    "            num_beams=6,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Initialize advanced RAG\n",
    "advanced_rag = AdvancedIdeaRAG(ideas)\n",
    "\n",
    "# Example usage\n",
    "response = advanced_rag.generate_response(\"What's connected with ambient music?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated New Ideas:\n",
      "1. Adaptive Fashion\n",
      "2. kind of like revising your look\n",
      "3. Adaptive\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class SelfImprovingRAG(AdvancedIdeaRAG):\n",
    "    def __init__(self, ideas):\n",
    "        super().__init__(ideas)\n",
    "\n",
    "    def generate_related_questions(self, answer, num_questions=3):\n",
    "        \"\"\"Generates follow-up questions based on the given answer.\"\"\"\n",
    "        prompt = f\"Generate {num_questions} insightful follow-up questions related to this answer:\\n\\n{answer}\"\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.generator.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=100,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        questions = self.tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"\\n\")\n",
    "        return [q.strip() for q in questions if q.strip()]\n",
    "\n",
    "    def generate_and_expand_ideas(self, query, depth=2):\n",
    "        \"\"\"Asks follow-up questions to generate new ideas iteratively.\"\"\"\n",
    "        seen_questions = set()\n",
    "        new_ideas = []\n",
    "        \n",
    "        # Start with the user's query\n",
    "        questions = [query]\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            next_questions = []\n",
    "            for question in questions:\n",
    "                if question in seen_questions:\n",
    "                    continue\n",
    "                seen_questions.add(question)\n",
    "                \n",
    "                # Get an answer from RAG\n",
    "                answer = self.generate_response(question)\n",
    "                new_ideas.append(answer)\n",
    "                \n",
    "                # Generate new related questions\n",
    "                follow_ups = self.generate_related_questions(answer)\n",
    "                next_questions.extend(follow_ups)\n",
    "\n",
    "            questions = next_questions  # Move to the next level of questioning\n",
    "        \n",
    "        # Add new ideas to our idea database\n",
    "        self.ideas.extend(new_ideas)\n",
    "        self._prepare_hybrid_search()  # Re-index new ideas\n",
    "        \n",
    "        return new_ideas\n",
    "\n",
    "# Initialize self-improving RAG\n",
    "self_rag = SelfImprovingRAG(ideas)\n",
    "\n",
    "# Example usage: Start with a single question and expand\n",
    "new_ideas = self_rag.generate_and_expand_ideas(\"How does ambient music influence creativity?\", depth=3)\n",
    "\n",
    "print(\"\\nGenerated New Ideas:\")\n",
    "for idx, idea in enumerate(new_ideas, 1):\n",
    "    print(f\"{idx}. {idea}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize idea connections using networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_idea_graph(ideas):\n",
    "    G = nx.Graph()\n",
    "    for idx, idea in enumerate(ideas):\n",
    "        G.add_node(f\"Idea {idx}\", label=idea[:50]+\"...\")\n",
    "        if idx > 0:\n",
    "            G.add_edge(f\"Idea {idx-1}\", f\"Idea {idx}\", weight=random.random())\n",
    "    \n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=2000, font_size=8)\n",
    "    plt.title(\"Idea Evolution Graph\")\n",
    "    plt.show()\n",
    "\n",
    "visualize_idea_graph(new_ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dee4505f0043959a8daeeadea6e0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db0d13383024029b4ccb4f11a39aa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa300dada3841769f6f4a429caef883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273f07efdc8d4c5283b692a951194138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bba4f37a93d495aa672cbae2b984efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee1080872864ef5b33850c68ca261c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fda4ddaa974e9c86cefc7a3511d68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea: Adaptive Fashion...\n",
      "Scores: {'coherence': 0.02, 'novelty_score': 0.7117825277229045}\n",
      "\n",
      "Idea: kind of like revising your look...\n",
      "Scores: {'coherence': 0.06, 'novelty_score': 0.8640364921326774}\n",
      "\n",
      "Idea: Adaptive...\n",
      "Scores: {'coherence': 0.01, 'novelty_score': 0.9434542762146164}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add quality assessment metrics\n",
    "from transformers import pipeline\n",
    "\n",
    "class IdeaEvaluator:\n",
    "    def __init__(self):\n",
    "        self.coherence_checker = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
    "        \n",
    "    def evaluate_idea(self, idea):\n",
    "        coherence_score = len(idea.split()) / 100  # Simple word count metric\n",
    "        return {\n",
    "            \"coherence\": min(coherence_score, 1.0),\n",
    "            \"novelty_score\": random.uniform(0.7, 0.95)\n",
    "        }\n",
    "\n",
    "evaluator = IdeaEvaluator()\n",
    "for idea in new_ideas[-5:]:\n",
    "    print(f\"Idea: {idea[:80]}...\")\n",
    "    print(f\"Scores: {evaluator.evaluate_idea(idea)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
