{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install required libraries\n",
    "# pip install openai pinecone-client sentence-transformers rank-bm25 transformers\n",
    "\n",
    "# 2. Imports\n",
    "import openai\n",
    "import pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "# 3. Initialize components\n",
    "OPENAI_API_KEY = \"your_key\"\n",
    "PINECONE_API_KEY = \"your_key\"\n",
    "PINECONE_ENV = \"your_env\"\n",
    "PINECONE_INDEX = \"your_index\"\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "index = pinecone.Index(PINECONE_INDEX)\n",
    "\n",
    "# 4. Data Ingestion Pipeline\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# 5. Hybrid Retrieval Model\n",
    "retriever = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 6. RAG Techniques Implementation\n",
    "class AdvancedRAG:\n",
    "    def __init__(self):\n",
    "        # Technique 1: Hybrid Search (Vector + Keyword)\n",
    "        self.bm25 = None\n",
    "        \n",
    "        # Technique 2: Query Expansion\n",
    "        self.expander = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "        self.expand_tokenizer = AutoTokenizer.from_pretrained(\"google/t5-small-ssm-nq\")\n",
    "        \n",
    "        # Technique 7: Dynamic Chunking\n",
    "        self.chunk_size = 512\n",
    "        \n",
    "    # Technique 3: Reciprocal Rank Fusion\n",
    "    def _rrf(self, scores):\n",
    "        return [1/(60 + rank) for rank in scores]\n",
    "    \n",
    "    # Technique 4: Query Expansion\n",
    "    def expand_query(self, query):\n",
    "        inputs = self.expand_tokenizer.encode(\"expand query: \" + query, return_tensors=\"pt\")\n",
    "        outputs = self.expander.generate(inputs, max_length=50)\n",
    "        return self.expand_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Technique 5: Semantic Chunking\n",
    "    def dynamic_chunking(self, text):\n",
    "        # Implement content-aware chunking\n",
    "        sentences = text.split('. ')\n",
    "        current_chunk = []\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) < self.chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "            else:\n",
    "                yield '. '.join(current_chunk)\n",
    "                current_chunk = [sentence]\n",
    "        if current_chunk:\n",
    "            yield '. '.join(current_chunk)\n",
    "    \n",
    "    # Technique 6: Contextual Compression\n",
    "    def compress_context(self, context, query):\n",
    "        prompt = f\"Relevant information for {query}: {context}\"\n",
    "        return openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=150\n",
    "        ).choices[0].text\n",
    "    \n",
    "    # Technique 8: Multi-Hop Retrieval\n",
    "    def iterative_retrieval(self, query, hops=2):\n",
    "        current_query = query\n",
    "        for _ in range(hops):\n",
    "            results = self.retrieve(current_query)\n",
    "            current_query += \" \" + \" \".join(results[:2])\n",
    "        return results\n",
    "    \n",
    "    # Technique 9: Diverse Retrieval\n",
    "    def diverse_retrieval(self, query, num_results=5):\n",
    "        vectors = retriever.encode([query])\n",
    "        results = index.query(vectors, top_k=num_results*2, include_metadata=True)\n",
    "        return self._deduplicate(results)[:num_results]\n",
    "    \n",
    "    # Technique 10: Real-time Update\n",
    "    def update_index(self, new_data):\n",
    "        chunks = list(self.dynamic_chunking(new_data))\n",
    "        embeddings = retriever.encode(chunks)\n",
    "        index.upsert([(str(id), vec, {\"text\": chunk}) for id, (vec, chunk) in enumerate(zip(embeddings, chunks))])\n",
    "    \n",
    "    def retrieve(self, query):\n",
    "        # Hybrid search implementation\n",
    "        expanded_query = self.expand_query(query)\n",
    "        \n",
    "        # Vector search\n",
    "        vector_results = index.query(\n",
    "            vector=retriever.encode(expanded_query).tolist(),\n",
    "            top_k=10,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        \n",
    "        # BM25 search\n",
    "        if self.bm25 is None:\n",
    "            corpus = [item['metadata']['text'] for item in index.fetch(ids=[]).matches]\n",
    "            tokenized_corpus = [doc.split() for doc in corpus]\n",
    "            self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        tokenized_query = expanded_query.split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Combine results using RRF\n",
    "        combined = self._rrf([item['score'] for item in vector_results.matches]) + self._rrf(bm25_scores)\n",
    "        return sorted(zip(combined, corpus), key=lambda x: x[0], reverse=True)[:10]\n",
    "\n",
    "# 7. Generation Component\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# 8. Full Pipeline\n",
    "def rag_pipeline(query):\n",
    "    rag = AdvancedRAG()\n",
    "    context = rag.retrieve(query)\n",
    "    compressed_context = rag.compress_context(context, query)\n",
    "    return generate_answer(query, compressed_context)\n",
    "\n",
    "# 9. Usage\n",
    "result = rag_pipeline(\"Your question here\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
