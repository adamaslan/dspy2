{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb0433c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dspy' has no attribute 'LM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLM\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdolphin-phi:2.7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dspy\u001b[38;5;241m.\u001b[39mconfigure(lm\u001b[38;5;241m=\u001b[39mlm)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'LM'"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "dspy.LM(model=\"dolphin-phi:2.7b\")\n",
    "dspy.configure(lm=lm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfddffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-9484c266-d2b0-48ca-8a28-186c4cb6162e', created=1752787670, model='ollama_chat/dolphin-phi:2.7b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! I am an artificial intelligence designed to assist with various tasks and provide information when needed. I can help you with your queries, solve problems, or simply have a conversation with you. My aim is to be helpful and make your life easier. How may I be of service to you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=61, prompt_tokens=33, total_tokens=94, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama_chat/dolphin-phi:2.7b\", \n",
    "    messages=[{ \"content\": \"respond in 20 words. who are you?\",\"role\": \"user\"}], \n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7cf7578",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dspy' has no attribute 'LM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlitellm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m completion\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLiteLLMLM\u001b[39;00m(\u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLM\u001b[49m):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom DSPy Language Model using LiteLLM\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama/dolphin-phi:2.7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'LM'"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from litellm import completion\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class LiteLLMLM(dspy.LM):\n",
    "    \"\"\"Custom DSPy Language Model using LiteLLM\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"ollama/dolphin-phi:2.7b\", **kwargs):\n",
    "        self.model = model\n",
    "        self.kwargs = kwargs\n",
    "        super().__init__(model)\n",
    "    \n",
    "    def generate(self, prompt: str, **kwargs) -> List[str]:\n",
    "        \"\"\"Generate response using LiteLLM\"\"\"\n",
    "        messages = [{\"content\": prompt, \"role\": \"user\"}]\n",
    "        \n",
    "        response = completion(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            **{**self.kwargs, **kwargs}\n",
    "        )\n",
    "        \n",
    "        return [response.choices[0].message.content]\n",
    "    \n",
    "    def __call__(self, prompt: str, **kwargs) -> List[str]:\n",
    "        \"\"\"DSPy interface method\"\"\"\n",
    "        return self.generate(prompt, **kwargs)\n",
    "\n",
    "# Initialize the custom LM\n",
    "lm = LiteLLMLM(model=\"ollama_chat/dolphin-phi:2.7b\")\n",
    "\n",
    "# Configure DSPy to use our custom LM\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define a simple signature for your task\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions concisely\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"A concise answer in about 20 words\")\n",
    "\n",
    "# Create Chain of Thought module\n",
    "cot = dspy.ChainOfThought(BasicQA)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Use Chain of Thought reasoning\n",
    "    response = cot(question=\"Who are you?\")\n",
    "    print(f\"Answer: {response.answer}\")\n",
    "    \n",
    "    # You can also access the reasoning chain if available\n",
    "    if hasattr(response, 'rationale'):\n",
    "        print(f\"Reasoning: {response.rationale}\")\n",
    "    \n",
    "    # Example with different question\n",
    "    response2 = cot(question=\"What is the capital of France?\")\n",
    "    print(f\"Answer: {response2.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d99c679",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dspy' has no attribute 'Signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Note: Run the Ollama configuration cell first!\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFactJudge\u001b[39;00m(\u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSignature\u001b[49m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Judge if the answer is factually correct based on the context.\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     context \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mInputField(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext for the prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dspy' has no attribute 'Signature'"
     ]
    }
   ],
   "source": [
    "# part 2\n",
    "import dspy\n",
    "import random\n",
    "\n",
    "# Note: Run the Ollama configuration cell first!\n",
    "\n",
    "class FactJudge(dspy.Signature):\n",
    "    \"\"\"Judge if the answer is factually correct based on the context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"Context for the prediction\")\n",
    "    question = dspy.InputField(desc=\"Question to be answered\")\n",
    "    answer = dspy.InputField(desc=\"Answer for the question\")\n",
    "    factually_correct: bool = dspy.OutputField(desc=\"Is the answer factually correct based on the context?\")\n",
    "\n",
    "class QASystem(dspy.Signature):\n",
    "    \"\"\"Answer questions based on given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"Context to answer from\")\n",
    "    question = dspy.InputField(desc=\"Question to answer\")\n",
    "    answer = dspy.OutputField(desc=\"Answer to the question\")\n",
    "\n",
    "# Initialize the judge with Gemma 2 2b\n",
    "judge = dspy.ChainOfThought(FactJudge)\n",
    "\n",
    "# Initialize QA system with Gemma 2 2b\n",
    "qa_system = dspy.ChainOfThought(QASystem)\n",
    "\n",
    "def factuality_metric(example, pred):\n",
    "    # Use the configured Gemma 2 2b model for judging\n",
    "    factual = judge(context=example.context, question=example.question, answer=pred.answer)\n",
    "    return factual.factually_correct\n",
    "\n",
    "# Sample data for testing\n",
    "sample_examples = [\n",
    "    {\n",
    "        \"context\": \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. The tower is 330 metres (1,083 ft) tall, about the same height as an 81-storey building.\",\n",
    "        \"question\": \"How tall is the Eiffel Tower?\",\n",
    "        \"answer\": \"330 metres\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Python is a high-level, interpreted programming language. It was created by Guido van Rossum and first released in 1991. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\",\n",
    "        \"question\": \"Who created Python?\",\n",
    "        \"answer\": \"Guido van Rossum\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials. It was built along the northern borders of China to protect against invasions. The wall stretches over 13,000 miles.\",\n",
    "        \"question\": \"How long is the Great Wall of China?\",\n",
    "        \"answer\": \"Over 13,000 miles\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to dspy.Example objects\n",
    "examples = [dspy.Example(**ex) for ex in sample_examples]\n",
    "\n",
    "def run_qa_demo():\n",
    "    print(\"=== DSPy Fact-Checking Demo ===\")\n",
    "    print(\"Using Gemma 2 2b for both QA and fact-checking\\n\")\n",
    "    \n",
    "    # Test with a new question\n",
    "    test_context = \"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity. He received the Nobel Prize in Physics in 1921 for his explanation of the photoelectric effect.\"\n",
    "    test_question = \"What did Einstein win the Nobel Prize for?\"\n",
    "    \n",
    "    print(f\"Context: {test_context}\")\n",
    "    print(f\"Question: {test_question}\")\n",
    "    \n",
    "    # Generate answer using Gemma 2 2b\n",
    "    prediction = qa_system(context=test_context, question=test_question)\n",
    "    generated_answer = prediction.answer\n",
    "    \n",
    "    print(f\"Generated Answer: {generated_answer}\")\n",
    "    \n",
    "    # Check factuality using Gemma 2 2b\n",
    "    fact_check = judge(\n",
    "        context=test_context, \n",
    "        question=test_question, \n",
    "        answer=generated_answer\n",
    "    )\n",
    "    is_factual = fact_check.factually_correct\n",
    "    reasoning = fact_check.reasoning if hasattr(fact_check, 'reasoning') else \"No reasoning provided\"\n",
    "    \n",
    "    print(f\"Factually Correct: {is_factual}\")\n",
    "    print(f\"Reasoning: {reasoning}\")\n",
    "    \n",
    "    # Test the metric function with sample examples\n",
    "    print(\"\\n=== Testing Factuality Metric ===\")\n",
    "    for i, example in enumerate(examples[:2]):  # Test first 2 examples\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Expected Answer: {example.answer}\")\n",
    "        \n",
    "        # Create a prediction object\n",
    "        pred = dspy.Example(answer=example.answer)\n",
    "        \n",
    "        # Test the metric\n",
    "        is_factual = factuality_metric(example, pred)\n",
    "        print(f\"Factually Correct: {is_factual}\")\n",
    "\n",
    "def test_with_wrong_answer():\n",
    "    print(\"\\n=== Testing with Incorrect Answer ===\")\n",
    "    wrong_example = dspy.Example(\n",
    "        context=\"The capital of France is Paris. Paris is located in the north-central part of France.\",\n",
    "        question=\"What is the capital of France?\",\n",
    "        answer=\"London\"  # Wrong answer\n",
    "    )\n",
    "    \n",
    "    pred = dspy.Example(answer=\"London\")\n",
    "    is_factual = factuality_metric(wrong_example, pred)\n",
    "    print(f\"Question: {wrong_example.question}\")\n",
    "    print(f\"Wrong Answer: London\")\n",
    "    print(f\"Factually Correct: {is_factual}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_qa_demo()\n",
    "        test_with_wrong_answer()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Make sure you have:\")\n",
    "        print(\"1. Run the Ollama configuration cell first\")\n",
    "        print(\"2. Ollama running: ollama serve\")\n",
    "        print(\"3. Gemma 2 2b model: ollama pull gemma2:2b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
