{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3490d7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HFModel' from 'dspy' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Import specific components from the dspy library\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdspy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     HFModel,  \u001b[38;5;66;03m# Changed from HuggingFaceClient\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     ChainOfThought,\n\u001b[1;32m      9\u001b[0m     Signature,\n\u001b[1;32m     10\u001b[0m     InputField,\n\u001b[1;32m     11\u001b[0m     OutputField,\n\u001b[1;32m     12\u001b[0m     Module,\n\u001b[1;32m     13\u001b[0m     Prediction,\n\u001b[1;32m     14\u001b[0m     configure,\n\u001b[1;32m     15\u001b[0m     Example\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# --- LLM Configuration ---\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 1. Configure the Language Model (LLM)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Using 'google/flan-t5-small' (~300 MB) for local execution.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# The class is now HFModel, not HuggingFaceClient.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m llm \u001b[38;5;241m=\u001b[39m HFModel(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoogle/flan-t5-small\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HFModel' from 'dspy' (unknown location)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Corrected: Import HFModel from its specific submodule\n",
    "from dspy.models.hf import HFModel\n",
    "\n",
    "# Import other necessary DSPy components\n",
    "from dspy import (\n",
    "    ChainOfThought,\n",
    "    Signature,\n",
    "    InputField,\n",
    "    OutputField,\n",
    "    Module,\n",
    "    Prediction,\n",
    "    configure,\n",
    "    Example\n",
    ")\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# 1. Configure the Language Model (LLM)\n",
    "# This will now use the locally cached 'google/flan-t5-small' model.\n",
    "# No download will occur here if you've run the download_model.py script.\n",
    "llm = HFModel(model='google/flan-t5-small')\n",
    "\n",
    "# 2. Configure DSPy to use this LLM\n",
    "configure(lm=llm)\n",
    "\n",
    "# --- Data and Corpus ---\n",
    "MY_CORPUS_PASSAGES = [\n",
    "    \"1) Maximize the Beauty - fully channel the beauty with in. Maybe ask what makes this moment beautiful? See if beauty can be increased in every situation. MtB also could be taken as a use of reason and also a disciplining of the senses to focus on beauty (i.e. all the pretty flowers, all the pretty birds). (Update 3-16-21 -This is one of the first 4 points created because they were on the top of my mind when I first started this project. Maximize the Beauty and Full Expression still are some of the concepts I am most influenced by, create an ethos around, consider to be primary to my concept of virtue, and aim to implement as much as possible.) \",\n",
    "    \"2) Full Expression - it takes a lot of effort for one to understand who they are when they are comfortable and how to channel the most real expressions of themselves what holds people back? Shyness, distraction (inability to focus on that which they want express)\",\n",
    "    \"3) Expect Rising - this means our expectations are constantly rising. Kind of in line with \\\"give em an inch they'll take a mile\\\" also related to law of diminishing returns -\\\"The law of diminishing returns states that in all productive processes, adding more of one factor of production, while holding all others constant (\\\"ceteris paribus\\\"), will at some point yield lower incremental per-unit returns.\\\" Tooo much ice cream too much cash\",\n",
    "    \"4) The Power of Pettiness - is the idea that pettiness is the destroyer of all people. That even the best of us can't truly be unaffected by the petty bullshit around us. They can however minimize its effects. In my own life I am obliterated by friends and coworker's snide remarks and judgments. How do I minimize its effects? By talking myself down, deep breaths, weed, alcohol, revenge...(this is the end of the first four points that were the strongest when I first started making these ideas on a small scrap of paper from the sides on union film sets.)\",\n",
    "    \"5) Various meditation - I am a big fan of meditation techniques that help train the mind from moment to moment. There are various techniques that help provide concentration and awareness in the passing moments within shorter intervals than traditional formal meditation. While using a timer to make sure you meditate for at least 10 minutes is helpful. A stopwatch to monitor your awareness of time and to also use time to monitor your concentration and awareness. A and C are essential to have Full Expression. Update 2-26-19 - Passive observation ala Krishnamurti. Just chill. Dont do anything. Just observe whatever is present. Don't react to thoughts just observe them. Don't interact with them. If they are mind blowing write them down. Then return to just chilling and observing.\"\n",
    "]\n",
    "\n",
    "# --- Signatures for DSPy Modules ---\n",
    "\n",
    "class GenerateAnswer(Signature):\n",
    "    \"\"\"Answer questions based on the provided context of personal philosophical ideas.\"\"\"\n",
    "    context = InputField(desc=\"Relevant personal philosophical ideas.\")\n",
    "    question = InputField(desc=\"The user's question.\")\n",
    "    answer = OutputField(desc=\"A thoughtful answer that aligns with the provided context.\")\n",
    "\n",
    "class IdeaConsistencyJudge(Signature):\n",
    "    \"\"\"Judge if the answer is consistent with the personal philosophical ideas and concepts in the context.\"\"\"\n",
    "    context = InputField(desc=\"Philosophical context from personal ideas corpus.\")\n",
    "    question = InputField(desc=\"Question about life philosophy or personal growth.\")\n",
    "    answer = InputField(desc=\"Answer to evaluate for consistency.\")\n",
    "    consistent_with_ideas: bool = OutputField(desc=\"Is the answer consistent with the philosophical ideas in the context?\")\n",
    "    explanation = OutputField(desc=\"Detailed reasoning for why the answer is or isn't consistent with the ideas.\")\n",
    "\n",
    "# --- DSPy Modules ---\n",
    "\n",
    "# Create the judge with Chain of Thought reasoning\n",
    "judge = ChainOfThought(IdeaConsistencyJudge)\n",
    "\n",
    "class MyIdeasRAGProgram(Module):\n",
    "    \"\"\"RAG program that retrieves from personal ideas corpus and includes context in predictions.\"\"\"\n",
    "    def __init__(self, corpus_passages: List[str]):\n",
    "        super().__init__()\n",
    "        self.corpus = corpus_passages\n",
    "        self.generator = ChainOfThought(GenerateAnswer)\n",
    "\n",
    "    def retrieve_relevant_ideas(self, question: str, top_k: int = 2) -> List[str]:\n",
    "        \"\"\"Simple keyword-based retrieval.\"\"\"\n",
    "        keywords = question.lower().split()\n",
    "        scored_passages = []\n",
    "        for passage in self.corpus:\n",
    "            score = sum(1 for keyword in keywords if keyword in passage.lower())\n",
    "            scored_passages.append((score, passage))\n",
    "        \n",
    "        scored_passages.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [passage for _, passage in scored_passages[:top_k]]\n",
    "\n",
    "    def forward(self, question: str):\n",
    "        relevant_ideas = self.retrieve_relevant_ideas(question)\n",
    "        context = \"\\n\\n\".join(relevant_ideas)\n",
    "        prediction = self.generator(context=context, question=question)\n",
    "        \n",
    "        return Prediction(\n",
    "            answer=prediction.answer,\n",
    "            context=context,\n",
    "            retrieved_ideas=relevant_ideas\n",
    "        )\n",
    "\n",
    "# --- Evaluation Logic ---\n",
    "\n",
    "def idea_consistency_metric(example: Example, pred: Prediction) -> dict:\n",
    "    \"\"\"Evaluate how well an answer aligns with the personal philosophy corpus.\"\"\"\n",
    "    judgment = judge(\n",
    "        context=example.context,\n",
    "        question=example.question,\n",
    "        answer=pred.answer\n",
    "    )\n",
    "    score = 1.0 if judgment.consistent_with_ideas else 0.0\n",
    "    return {\n",
    "        'consistent': judgment.consistent_with_ideas,\n",
    "        'explanation': judgment.explanation,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "# --- Main Application Pipeline ---\n",
    "\n",
    "class IdeasConsistencyPipeline:\n",
    "    \"\"\"A pipeline for evaluating answers against a personal philosophy.\"\"\"\n",
    "    def __init__(self, corpus_passages: List[str]):\n",
    "        self.rag_program = MyIdeasRAGProgram(corpus_passages)\n",
    "\n",
    "    def evaluate_answer(self, question: str, answer: str, context: str):\n",
    "        \"\"\"Evaluate a single answer against the ideas corpus.\"\"\"\n",
    "        eval_example = Example(question=question, context=context)\n",
    "        prediction = Prediction(answer=answer)\n",
    "        return idea_consistency_metric(eval_example, prediction)\n",
    "\n",
    "    def interactive_mode(self):\n",
    "        \"\"\"Interactive mode for testing custom questions.\"\"\"\n",
    "        print(\"\\n\\n🎮 Interactive Ideas Consistency Checker\")\n",
    "        print(\"Ask questions about life philosophy and get evaluated answers!\")\n",
    "        print(\"Model: google/flan-t5-small\")\n",
    "        print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "        while True:\n",
    "            question = input(\"💭 Your question: \").strip()\n",
    "            if question.lower() == 'quit':\n",
    "                print(\"Exiting interactive mode. Goodbye! 👋\")\n",
    "                break\n",
    "            if not question:\n",
    "                continue\n",
    "\n",
    "            pred = self.rag_program(question)\n",
    "            print(f\"\\n🤖 Answer: {pred.answer}\")\n",
    "\n",
    "            result = self.evaluate_answer(question, pred.answer, pred.context)\n",
    "\n",
    "            print(f\"\\n✅ Consistent with your ideas: {result['consistent']}\")\n",
    "            print(f\"📊 Consistency score: {result['score']}\")\n",
    "            print(f\"💭 Judge's reasoning: {result['explanation']}\")\n",
    "            print(f\"📚 Context used: {pred.context[:150]}...\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "# --- Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = IdeasConsistencyPipeline(MY_corpus_passages)\n",
    "    pipeline.interactive_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dd6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Starting download for model: google/flan-t5-small\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c10e679d0a4401aa8647b84256b1933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aa37b41e574bc5b3fa833c0decedee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657bc7d9a0164c428deb80e3663d67f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30632ffda4dc4aaf9d0cbd928d7ca479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)66c868570469993386a093bca18aff3c1e13b065:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b500dd40a646118b7c8aee43a93fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)4d0ad38fca5173b09393b22b864ab8a55ba03d7c:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb7dad591594884b21f6d18d868f262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3513c8678e2463996e8e9c9c984bad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c700bf20a54abd81923bc623a69176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# The Hugging Face model we want to use\n",
    "MODEL_ID = \"google/flan-t5-small\"\n",
    "\n",
    "def download_hf_model():\n",
    "    \"\"\"\n",
    "    Downloads and caches the model and tokenizer files from Hugging Face.\n",
    "    \"\"\"\n",
    "    print(f\"📥 Starting download for model: {MODEL_ID}\")\n",
    "    \n",
    "    # This function downloads the entire model repository to the local cache.\n",
    "    # The cache is typically located at ~/.cache/huggingface/hub/\n",
    "    snapshot_download(repo_id=MODEL_ID)\n",
    "    \n",
    "    print(f\"✅ Download complete. Model '{MODEL_ID}' is cached locally.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To run this script and download the model, open your terminal and type:\n",
    "    # pip install huggingface_hub\n",
    "    # python download_model.py\n",
    "    download_hf_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
