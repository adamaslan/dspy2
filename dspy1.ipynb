{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Imports, CSV Loading, and Semantic Search Setup\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import dspy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "\n",
    "# Optimized for M2 Mac (or CPU)\n",
    "huggingface_model = 'distilgpt2'\n",
    "csv_path = \"./151_ideas_updated.csv\"\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(csv_path, on_bad_lines='skip')\n",
    "    data = data.dropna().reset_index(drop=True)\n",
    "    if 'Ideas' not in data.columns:\n",
    "        raise ValueError(\"Column 'Ideas' not found in CSV.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: CSV file not found at {csv_path}\")\n",
    "    exit(1)\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Semantic Search Setup (do this ONCE)\n",
    "try:\n",
    "    embedder = SentenceTransformer('all-mpnet-base-v2')\n",
    "    idea_embeddings = embedder.encode(data['Ideas'].tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up embeddings: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "def get_relevant_context(query, top_k=2):\n",
    "    # Encode the query using the embedder\n",
    "    query_embedding = embedder.encode(query)\n",
    "    \n",
    "    # Calculate cosine similarities with precomputed idea embeddings\n",
    "    similarities = util.cos_sim(query_embedding, idea_embeddings)[0]\n",
    "    \n",
    "    # Sort indices by descending similarity and fetch the top_k results\n",
    "    top_indices = np.argsort(-similarities.numpy())[:top_k]  # Negate for descending order\n",
    "    \n",
    "    # Retrieve and return the top ideas from the data\n",
    "    return [data.loc[i, 'Ideas'] for i in top_indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Model Loading, DSPy Setup, and Chatbot Signature\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(huggingface_model)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        huggingface_model,\n",
    "        torch_dtype=torch.float16 if device == \"mps\" else torch.float32,\n",
    "        low_cpu_mem_usage=True\n",
    "    ).to(device)\n",
    "    text_generator = pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Model loading error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit(1)\n",
    "\n",
    "# Custom Language Model for DSPy\n",
    "class SimpleLLM(dspy.LM):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "    def __call__(self, prompt, max_length=150, num_return_sequences=1, do_sample=True, temperature=0.7):\n",
    "        try:\n",
    "            responses = self.generator(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                do_sample=do_sample,\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return responses[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"I'm having trouble generating a response.\"\n",
    "\n",
    "\n",
    "# Define chatbot signature with CoT prompting\n",
    "class ChatbotSignature(dspy.Signature):\n",
    "    \"\"\"Generate a helpful and concise response to a user's query, using context from the dataset and thinking step by step.\"\"\"\n",
    "    query = dspy.InputField(desc=\"The user's query.\")\n",
    "    context = dspy.InputField(desc=\"Relevant context from the dataset.\")\n",
    "    response = dspy.OutputField(desc=\"A helpful and relevant answer, explained step by step.\")\n",
    "\n",
    "# Configure DSPy\n",
    "dspy.settings.configure(lm=SimpleLLM(text_generator))\n",
    "\n",
    "# Create prediction module\n",
    "chatbot = dspy.Predict(ChatbotSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\n",
      "You: Hello, what is DSPy?\n",
      "Chat error: 'SimpleLLM' object has no attribute 'kwargs'\n",
      "You: Tell me more about full expression.\n",
      "Chat error: 'SimpleLLM' object has no attribute 'kwargs'\n",
      "You: exit\n",
      "DSPy Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "#3 Interactive Chat Loop\n",
    "def chat():\n",
    "    print(\"DSPy Chatbot: Hello! I'm ready to help. Type 'exit' to quit.\")\n",
    "    \n",
    "    simulated_inputs = iter([\n",
    "        \"Hello, what is DSPy?\",  # Example inputs\n",
    "        \"Tell me more about full expression.\",\n",
    "        \"exit\"\n",
    "    ])\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = next(simulated_inputs, 'exit')\n",
    "            print(f\"You: {user_input}\")\n",
    "            \n",
    "            if user_input.lower() == 'exit':\n",
    "                print(\"DSPy Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            relevant_context = get_relevant_context(user_input)\n",
    "            context_string = \"\\n\".join(relevant_context)\n",
    "\n",
    "            # Construct prompt\n",
    "            enhanced_prompt = f\"\"\"Context from dataset:\n",
    "{context_string}\n",
    "\n",
    "User query: {user_input}\n",
    "\n",
    "Let's think step by step to provide a helpful response:\"\"\"\n",
    "\n",
    "            # Generate response\n",
    "            response = chatbot(query=user_input, context=context_string)\n",
    "            print(\"DSPy Chatbot:\", response.response)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Chat error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
