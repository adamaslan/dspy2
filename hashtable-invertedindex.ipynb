{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E - Alpha updated \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Check current directory and files\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "print(\"Files in Directory:\", os.listdir())\n",
    "\n",
    "# Attempt to load CSV\n",
    "try:\n",
    "    df = pd.read_csv('articles.csv', quotechar='\"', lineterminator='\\n', skip_blank_lines=True)\n",
    "    print(\"CSV loaded successfully. First few rows:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'articles.csv' not found. Check the file location.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error parsing CSV: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure required column exists\n",
    "if 'articles' not in df.columns:\n",
    "    print(\"Error: Column 'articles' not found in the CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Proceed with processing...\n",
    "\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Extract the original numbering (handling missing numbers correctly)\n",
    "df['Original_Number'] = df['article'].str.extract(r'^(\\d+)\\)').fillna(-1).astype(int)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing (removing the number first)\n",
    "df['articles_without_number'] = df['articles'].str.replace(r'^\\d+\\)\\s*', '', regex=True)\n",
    "df['Cleaned_articles'] = df['articles_without_number'].apply(preprocess_text)\n",
    "\n",
    "# Create the hash table (handling missing numbers correctly)\n",
    "hash_table = {}\n",
    "for _, row in df.iterrows():\n",
    "    key = row['Original_Number']\n",
    "    hash_table[key] = {\n",
    "        \"original_content\": row['articles'],\n",
    "        \"cleaned_content\": row['Cleaned_articles']\n",
    "    }\n",
    "\n",
    "# Step 2: Creating an Inverted Index\n",
    "inverted_index = defaultdict(list)\n",
    "for doc_id, data in hash_table.items():\n",
    "    if doc_id != -1:  # ignore the non numbered entries\n",
    "        for word in data[\"cleaned_content\"].split():\n",
    "            inverted_index[word].append(doc_id)\n",
    "\n",
    "# Example Query\n",
    "query = \"beauty\"\n",
    "query = preprocess_text(query)\n",
    "query_words = query.split()\n",
    "\n",
    "# Retrieval using Inverted Index\n",
    "retrieved_docs = set()\n",
    "for word in query_words:\n",
    "    if word in inverted_index:\n",
    "        retrieved_docs.update(inverted_index[word])\n",
    "\n",
    "# Print the hash table (improved formatting, with original number)\n",
    "print(\"\\nHash Table:\")\n",
    "for key, value in hash_table.items():\n",
    "    if key != -1:\n",
    "        print(f\"ID: {key}\")\n",
    "        print(f\"  Original Content: {value['original_content']}\")\n",
    "        print(f\"  Cleaned Content: {value['cleaned_content']}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "# Output Retrieved Documents (using original content and number)\n",
    "print(\"\\nDocuments containing the query words:\")\n",
    "for doc_id in retrieved_docs:\n",
    "    if doc_id != -1:  # important to avoid printing non existent keys\n",
    "        print(f\"Original Number: {doc_id}, Content: {hash_table[doc_id]['original_content']}\")\n",
    "\n",
    "# Example of how to use the hash table for embedding creation (after pre-filtering)\n",
    "docs_to_embed = [hash_table[doc_id]['cleaned_content'] for doc_id in retrieved_docs if doc_id != -1]\n",
    "print(f\"\\nDocuments to be embedded: {docs_to_embed}\")\n",
    "\n",
    "\n",
    "# Define the directory for storing hash tables\n",
    "output_dir = \"tablisi\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# Create a unique filename using a timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "filename = f\"hash_table_{timestamp}.json\"\n",
    "output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "# Write the hash table to the JSON file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hash_table, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Hash table saved to: {output_path}\")\n",
    "print(f\"\\nHash table written to: {output_path}\")  # confirmation message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParserError: Error tokenizing data. C error: Expected 1 fields in line 8, saw 2\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     exit()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Ensure the required column exists\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Column \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in the CSV file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m     exit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# just hash module\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize words\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Step 1: Load data from CSV\n",
    "try:\n",
    "    df = pd.read_csv('articles.csv', quotechar='\"', lineterminator='\\n', skip_blank_lines=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'articles.csv' not found.\")\n",
    "    exit()\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"ParserError: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the required column exists\n",
    "if 'articles' not in df.columns:\n",
    "    print(\"Error: Column 'articles' not found in the CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Extract original numbering if present (adjust logic if numbering isn't relevant)\n",
    "df['Original_Number'] = df['articles'].str.extract(r'^(\\d+)\\)').fillna(-1).astype(int)\n",
    "\n",
    "# Preprocess the 'articles' column\n",
    "df['Cleaned_articles'] = df['articles'].apply(preprocess_text)\n",
    "\n",
    "# Create the hash table\n",
    "hash_table = {\n",
    "    idx: {\n",
    "        \"original_content\": row['articles'],\n",
    "        \"cleaned_content\": row['Cleaned_articles']\n",
    "    }\n",
    "    for idx, row in df.iterrows()\n",
    "}\n",
    "\n",
    "# Save the hash table to a JSON file\n",
    "output_dir = \"tablisi\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"hash_table.json\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hash_table, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Hash table saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just query and inverted indexing\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load the hash table from the JSON file\n",
    "hash_table_path = \"tablisi/hash_table_<timestamp>.json\"  # Replace <timestamp> with the correct file\n",
    "with open(hash_table_path, 'r', encoding='utf-8') as f:\n",
    "    hash_table = json.load(f)\n",
    "\n",
    "# Rebuild the inverted index\n",
    "inverted_index = defaultdict(list)\n",
    "for doc_id, data in hash_table.items():\n",
    "    doc_id = int(doc_id)\n",
    "    if doc_id != -1:  # Ignore non-numbered entries\n",
    "        for word in data[\"cleaned_content\"].split():\n",
    "            inverted_index[word].append(doc_id)\n",
    "\n",
    "# Preprocessing function for queries\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Example Query\n",
    "query = \"beauty\"\n",
    "query = preprocess_text(query)\n",
    "query_words = query.split()\n",
    "\n",
    "# Retrieval using Inverted Index\n",
    "retrieved_docs = set()\n",
    "for word in query_words:\n",
    "    if word in inverted_index:\n",
    "        retrieved_docs.update(inverted_index[word])\n",
    "\n",
    "# Output Retrieved Documents\n",
    "print(\"\\nDocuments containing the query words:\")\n",
    "for doc_id in retrieved_docs:\n",
    "    if doc_id != -1:  # Ignore invalid keys\n",
    "        print(f\"Original Number: {doc_id}, Content: {hash_table[str(doc_id)]['original_content']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
